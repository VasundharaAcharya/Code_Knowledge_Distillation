{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41xicrhV9BUl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve, IsotonicRegression\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import brier_score_loss, log_loss\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "\n",
        "def softmax(logits):\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "\n",
        "# We start with the initial probabilities from the uncalibrated model\n",
        "y_probs = softmax(all_train_logits_df.to_numpy())\n",
        "\n",
        "#suitable for multiclass here\n",
        "lb = LabelBinarizer()\n",
        "y_true_binarized = lb.fit_transform(y_train_df)\n",
        "num_classes = y_probs.shape[1]\n",
        "\n",
        "# #Using cross validation is the best practice\n",
        "\n",
        "n_splits = 5 # Using 5 splits, this can be changed. It has to be similar to the one used for temperature scaling to maintain fairness.\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "final_calibrated_probs_iso = np.zeros_like(y_probs)\n",
        "\n",
        "print(f\"Starting {n_splits}-fold cross-validation to generate Isotonic Regression probabilities...\")\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(y_probs)):\n",
        "    print(f\"--- Processing Fold {fold+1}/{n_splits} ---\")\n",
        "\n",
        "    ## Split the initial probabilities and true labels into training and validation sets for this fold\n",
        "    y_probs_train, y_probs_val = y_probs[train_index], y_probs[val_index]\n",
        "    y_true_train, y_true_val = y_true_binarized[train_index], y_true_binarized[val_index]\n",
        "\n",
        "    ## This will hold the calibrated probabilities for the validation set of THIS FOLD\n",
        "    calibrated_probs_val = np.zeros_like(y_probs_val)\n",
        "\n",
        "    ## Train a separate Isotonic Regression model for each class on the TRAIN portion of the fold\n",
        "    for i in range(num_classes):\n",
        "        ir = IsotonicRegression(out_of_bounds='clip')\n",
        "\n",
        "\n",
        "        ir.fit(y_probs_train[:, i], y_true_train[:, i])\n",
        "\n",
        "\n",
        "        calibrated_probs_val[:, i] = ir.transform(y_probs_val[:, i])\n",
        "\n",
        "     ##NOTE: Renormalize them before using-using your method (not shown here)\n",
        "    final_calibrated_probs_iso[val_index] = calibrated_probs_val\n",
        "\n",
        "print(\"\\nCross-validation complete. Final calibrated probabilities have been generated.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def stratified_brier_score_per_class(y_true_binarized, y_prob):\n",
        "    brier_scores = []\n",
        "    y_true_flat = y_true_binarized.argmax(axis=1) # Convert from one-hot to class indices\n",
        "    for i in range(y_prob.shape[1]):\n",
        "        pos_indices = (y_true_flat == i)\n",
        "        neg_indices = (y_true_flat != i)\n",
        "\n",
        "        N_pos = np.sum(pos_indices)\n",
        "        N_neg = np.sum(neg_indices)\n",
        "\n",
        "        if N_pos > 0:\n",
        "            brier_score_pos = np.sum((1 - y_prob[pos_indices, i])**2) / N_pos\n",
        "        else:\n",
        "            brier_score_pos = 0\n",
        "\n",
        "        if N_neg > 0:\n",
        "\n",
        "            brier_score_neg = np.sum((y_prob[neg_indices, i])**2) / N_neg\n",
        "        else:\n",
        "            brier_score_neg = 0\n",
        "\n",
        "        stratified_brier = (brier_score_pos + brier_score_neg) / 2\n",
        "        brier_scores.append(stratified_brier)\n",
        "    return brier_scores\n",
        "\n",
        "\n",
        "print('\\n Final metrics for Isotonic Regression (from cross-validated probabilities)')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Compute final metrics using the fully populated `final_calibrated_probs_iso`\n",
        "\n",
        "final_classwise_log_losses = [log_loss(y_true_binarized[:, i], final_calibrated_probs_iso[:, i]) for i in range(num_classes)]\n",
        "\n",
        "## Print class-wise results\n",
        "for i in range(num_classes):\n",
        "    print(f'Class {i}:')\n",
        "    print(f'    Log-loss: {final_classwise_log_losses[i]:.6f}')\n",
        "    print(f'    Stratified Brier score: {final_stratified_brier_scores[i]:.6f}')\n",
        "\n",
        "# Print overall results, display the mean of log losses if needed.\n",
        "\n",
        "print(f'Overall mean Stratified Brier score: {np.mean(final_stratified_brier_scores):.6f}')\n",
        "\n",
        "\n",
        "\n",
        "def plot_calibration_curve(y_true_binarized, probs, title):\n",
        "    plt.figure(figsize=(13, 8))\n",
        "    y_true_flat = y_true_binarized.argmax(axis=1)\n",
        "    for i in range(probs.shape[1]):\n",
        "        prob_true, prob_pred = calibration_curve(y_true_flat == i, probs[:, i], n_bins=10)\n",
        "        plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel('Mean predicted probability', fontweight='bold', fontsize=24)\n",
        "    plt.ylabel('Fraction of positives', fontweight='bold', fontsize=24)\n",
        "    plt.title(title, fontweight='bold', fontsize=24)\n",
        "    plt.xticks(fontsize=24, fontweight='bold')\n",
        "    plt.yticks(fontsize=24, fontweight='bold')\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.show()\n",
        "\n",
        "##Call this function to plot the calibration plot\n",
        "plot_calibration_curve(y_true_binarized, final_calibrated_probs_iso, 'Calibration Plot - Isotonic Regression')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}