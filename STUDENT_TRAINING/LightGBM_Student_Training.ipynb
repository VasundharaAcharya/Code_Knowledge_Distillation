{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwDGgmfk8NGa"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split # Example import\n",
        "\n",
        "\n",
        "\n",
        "####Placeholder Data (REPLACE WITH YOUR ACTUAL DATA).\n",
        "\n",
        "X_train_df = pd.DataFrame(np.random.rand(100, 10))\n",
        "X_val_df = pd.DataFrame(np.random.rand(50, 10))\n",
        "X_test_df = pd.DataFrame(np.random.rand(50, 10))\n",
        "\n",
        "# True class labels\n",
        "y_train_df = pd.Series(np.random.randint(0, 3, 100))\n",
        "y_val_df = pd.Series(np.random.randint(0, 3, 50))\n",
        "y_test_df = pd.Series(np.random.randint(0, 3, 50))\n",
        "\n",
        "# Logits from the teacher model for a 3-class problem. It will have 3 columns.\n",
        "all_train_logits_df = pd.DataFrame(np.random.rand(100, 3))\n",
        "\n",
        "\n",
        "def softmax(logits):\n",
        "\n",
        "    e_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    return e_logits / e_logits.sum(axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def run_distillation_experiment(random_state, X_train, y_train, X_val, y_val, X_test, y_test, train_logits):\n",
        "\n",
        "    ##Trains a set of regressors to mimic teacher logits and evaluates the resulting student classifier.\n",
        "\n",
        "    models = []\n",
        "\n",
        "    ## Train one regressor for each logit column\n",
        "    for i in range(train_logits.shape[1]):\n",
        "        model = lgb.LGBMRegressor(\n",
        "            device_type='gpu', # Change to 'cpu' if no GPU\n",
        "            n_estimators=6,\n",
        "            num_leaves=50,\n",
        "            max_depth=12,\n",
        "            learning_rate=0.3,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        # Fit the model to predict the i-th logit\n",
        "        model.fit(X_train, train_logits.iloc[:, i])\n",
        "        models.append(model)\n",
        "\n",
        "    ## Predict logits for all datasets\n",
        "    pred_logits_train = np.array([m.predict(X_train) for m in models]).T\n",
        "    pred_logits_val = np.array([m.predict(X_val) for m in models]).T\n",
        "    pred_logits_test = np.array([m.predict(X_test) for m in models]).T\n",
        "\n",
        "    ## Convert predicted logits to class labels via softmax\n",
        "    class_labels_train = np.argmax(softmax(pred_logits_train), axis=1)\n",
        "    class_labels_val = np.argmax(softmax(pred_logits_val), axis=1)\n",
        "    class_labels_test = np.argmax(softmax(pred_logits_test), axis=1)\n",
        "\n",
        "    ## Calculate performance metrics\n",
        "    acc_train = accuracy_score(y_train, class_labels_train)\n",
        "    acc_val = accuracy_score(y_val, class_labels_val)\n",
        "    acc_test = accuracy_score(y_test, class_labels_test)\n",
        "\n",
        "    f1_train = f1_score(y_train, class_labels_train, average='weighted')\n",
        "    f1_val = f1_score(y_val, class_labels_val, average='weighted')\n",
        "    f1_test = f1_score(y_test, class_labels_test, average='weighted')\n",
        "\n",
        "    # Print the confusion matrix for this run for inspection. This step can be done if class wise performance needs to be inspected\n",
        "    print(f\"Confusion Matrix for seed {random_state}:\\n{confusion_matrix(y_test, class_labels_test)}\\n\")\n",
        "\n",
        "    return acc_train, acc_val, acc_test, f1_train, f1_val, f1_test\n",
        "\n",
        "\n",
        "##Running the experiment with multiple random seeds##\n",
        "results = []\n",
        "seeds = [20, 42, 100]\n",
        "for seed in seeds:\n",
        "    exp_results = run_distillation_experiment(\n",
        "        seed,\n",
        "        X_train_df, y_train_df,\n",
        "        X_val_df, y_val_df,\n",
        "        X_test_df, y_test_df,\n",
        "        all_train_logits_df\n",
        "    )\n",
        "    results.append(exp_results)\n",
        "\n",
        "## Aggregate and print final results, compute std if required#\n",
        "results_np = np.array(results)\n",
        "mean_results = results_np.mean(axis=0)\n",
        "\n",
        "\n",
        "print(\"--- Final Aggregated Results ---\")\n",
        "print(f\"Mean Accuracy (Train | Val | Test): {mean_results[0]:.4f} | {mean_results[1]:.4f} | {mean_results[2]:.4f}\")\n",
        ")\n",
        "\n",
        "print(f\"Mean F1-Score (Train | Val | Test): {mean_results[3]:.4f} | {mean_results[4]:.4f} | {mean_results[5]:.4f}\")\n"
      ]
    }
  ]
}