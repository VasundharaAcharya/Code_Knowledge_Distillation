{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kWdse498i36"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "######This function is to find the temperature that reduces the log loss.\n",
        "\n",
        "##Replace these with you data\n",
        "all_train_logits = all_train_logits_df.to_numpy()\n",
        "y_train = y_train_df.to_numpy().ravel()\n",
        "\n",
        "\n",
        "\n",
        "def temperature_scaled_softmax(logits, temperature):\n",
        "    scaled_logits = logits / temperature\n",
        "    exp_logits = np.exp(scaled_logits - np.max(scaled_logits, axis=-1, keepdims=True))\n",
        "    return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "def stratified_brier_score_per_class(y_true, y_prob):\n",
        "    brier_scores = []\n",
        "    for i in range(y_prob.shape[1]):\n",
        "        pos_indices = (y_true == i)\n",
        "        neg_indices = (y_true != i)\n",
        "\n",
        "        N_pos = np.sum(pos_indices)\n",
        "        N_neg = np.sum(neg_indices)\n",
        "\n",
        "        if N_pos > 0:\n",
        "            brier_score_pos = np.sum((1 - y_prob[pos_indices, i])**2) / N_pos\n",
        "        else:\n",
        "            brier_score_pos = 0\n",
        "\n",
        "        if N_neg > 0:\n",
        "            brier_score_neg = np.sum((y_prob[neg_indices, i])**2) / N_neg\n",
        "        else:\n",
        "            brier_score_neg = 0\n",
        "\n",
        "        stratified_brier = (brier_score_pos + brier_score_neg) / 2\n",
        "        brier_scores.append(stratified_brier)\n",
        "    return brier_scores\n",
        "\n",
        "def optimize_temperature(logits, labels):\n",
        "    def loss_to_minimize(temperature):\n",
        "        probs = temperature_scaled_softmax(logits, temperature)\n",
        "\n",
        "        return log_loss(labels, probs)\n",
        "\n",
        "    # Find the temperature that minimizes the loss function\n",
        "    result = minimize(loss_to_minimize, x0=1.5, bounds=[(0.1, 10.0)], method='L-BFGS-B')\n",
        "    return result.x[0]\n",
        "\n",
        "\n",
        "#Best practice is to use cross validation here\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "optimal_temperatures = []\n",
        "\n",
        "out_of_sample_calibrated_probs = np.zeros_like(all_train_logits)\n",
        "\n",
        "print(f\"--- Starting Corrected {n_splits}-Fold Cross-Validation ---\")\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(all_train_logits)):\n",
        "    train_logits, val_logits = all_train_logits[train_index], all_train_logits[val_index]\n",
        "    train_labels, val_labels = y_train[train_index], y_train[val_index]\n",
        "\n",
        "\n",
        "    optimal_temp = optimize_temperature(train_logits, train_labels)\n",
        "    optimal_temperatures.append(optimal_temp)\n",
        "    print(f\"Fold {fold+1}/{n_splits} | Optimal Temperature found on TRAIN set: {optimal_temp:.4f}\")\n",
        "\n",
        "\n",
        "    calibrated_val_probs = temperature_scaled_softmax(val_logits, optimal_temp)\n",
        "\n",
        "\n",
        "    out_of_sample_calibrated_probs[val_index] = calibrated_val_probs\n",
        "\n",
        "print(\"\\n--- Cross-Validation Complete ---\")\n",
        "\n",
        "##Final Robust Metrics and Analysis##\n",
        "\n",
        "\n",
        "print(\"\\n--- Final Metrics (Calculated from Robust Out-of-Sample Predictions) ---\")\n",
        "\n",
        "#Note: This temperature can also be used to calibrate the entire training set logits\n",
        "best_overall_temperature = np.mean(optimal_temperatures)\n",
        "print(f\"Average Optimal Temperature across all folds: {best_overall_temperature:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "final_log_losses = [log_loss((y_train == i).astype(int), out_of_sample_calibrated_probs[:, i]) for i in range(out_of_sample_calibrated_probs.shape[1])]\n",
        "final_stratified_brier_scores = stratified_brier_score_per_class(y_train,out_of_sample_calibrated_probs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "final_log_loss_overall = log_loss(y_train, out_of_sample_calibrated_probs)\n",
        "final_stratified_brier_scores = stratified_brier_score_per_class(y_train, out_of_sample_calibrated_probs)\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Overall Mean Stratified Brier Score: {np.mean(final_stratified_brier_scores):.4f}\")\n",
        "\n",
        "print('Final metrics with best temperature:')\n",
        "for i in range(len(final_log_losses)):\n",
        "    print(f'Class {i}:')\n",
        "    print(f'    Log-loss: {final_log_losses[i]:.4f}')\n",
        "    print(f'    Stratified Brier score: {final_stratified_brier_scores[i]:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "####if the plot is to be drawn##.\n",
        "def plot_calibration_curve(y_true, probs, title):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i in range(probs.shape[1]):\n",
        "        # Use y_true directly, no need for argmax if it's already 1D\n",
        "        prob_true, prob_pred = calibration_curve((y_true == i), probs[:, i], n_bins=10)\n",
        "        plt.plot(prob_pred, prob_true, marker='o', label=f'Class {i}')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "    plt.xlabel('Mean Predicted Probability', fontweight='bold', fontsize=24)\n",
        "    plt.ylabel('Fraction of Positives', fontweight='bold', fontsize=24)\n",
        "    plt.title(title, fontweight='bold', fontsize=24)\n",
        "    plt.xticks(fontsize=24, fontweight='bold')\n",
        "    plt.yticks(fontsize=24, fontweight='bold')\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "####call the function to plot the curve here\n",
        "plot_calibration_curve(y_train, out_of_sample_calibrated_probs, 'Calibration Curve (After Temperature Scaling)')"
      ]
    }
  ]
}